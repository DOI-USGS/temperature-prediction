{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geojson\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location of input data files\n",
    "input_dir = 'input_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment polylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in segment shapefile (simplified to 4 percent with mapshaper and smoothed in ArcGIS) as geodataframe\n",
    "segment_gdf = gpd.read_file(os.path.join(input_dir,'shapefiles/Segments_subset_4per_smooth.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delaware Bay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in NHD delaware bay shapefile (simplified to 0.6 percent with mapshaper and smoothed in ArcGIS) as geodataframe\n",
    "delaware_bay_gdf = gpd.read_file(os.path.join(input_dir,'shapefiles/NHDWaterbody_DelawareBay_pt6per_smooth.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in simplified reservoirs shapefile as geodataframe\n",
    "reservoirs = gpd.read_file(os.path.join(input_dir,'shapefiles/reservoirs_17per.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out four reservoirs that are not on model segments\n",
    "reservoirs = reservoirs[(reservoirs.GRAND_ID != 2212) & (reservoirs.GRAND_ID != 1591) & (reservoirs.GRAND_ID != 1584) & (reservoirs.GRAND_ID != 2242)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop index\n",
    "reservoirs = reservoirs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique drb sites (coordinate system = NAD1983 = EPSG 4269)\n",
    "unique_drb_sites = pd.read_csv(os.path.join(input_dir,'unique_drb_sites.csv'), index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_df_raw = pd.read_csv(os.path.join(input_dir,'obs_temp_drb.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Observations per year, by source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_annual_source = pd.read_csv(os.path.join(input_dir,'n_obs_per_year_source.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeled segment outflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_outflow = pd.read_csv(os.path.join(input_dir,'seg_outflow.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and structure data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean temperature observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NAs from seg_id_nat column\n",
    "obs_temp_df_cleaned = obs_temp_df_raw.loc[obs_temp_df_raw['seg_id_nat'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get # of unique segment ids\n",
    "obs_temp_seg_id_nat_unique = np.unique(obs_temp_df_cleaned['seg_id_nat'].tolist())\n",
    "len(obs_temp_seg_id_nat_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "obs_temp_df_cleaned['date'] = pd.to_datetime(obs_temp_df_cleaned['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find single erroneous observation value\n",
    "obs_temp_df_cleaned.loc[(obs_temp_df_cleaned['date'] == '2019-08-21') & (obs_temp_df_cleaned['seg_id_nat'] == 1764)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop that row from the dataframe\n",
    "obs_temp_df_cleaned = obs_temp_df_cleaned.drop(labels=307015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date as index and sort by date\n",
    "obs_temp_df_cleaned = obs_temp_df_cleaned.set_index(['date'], drop=True)\n",
    "obs_temp_df_cleaned = obs_temp_df_cleaned.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new copy of cleaned dataframe\n",
    "obs_temp_df = obs_temp_df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert segment id to integer\n",
    "obs_temp_df['seg_id_nat'] = obs_temp_df['seg_id_nat'].astype(int)\n",
    "# create year column based on year index\n",
    "obs_temp_df['year'] = obs_temp_df.index.year #.astype(int)\n",
    "# create year-month column\n",
    "obs_temp_df['year-month'] = obs_temp_df.index.to_period('M')\n",
    "# create month column\n",
    "obs_temp_df['month'] = obs_temp_df.index.month\n",
    "# for each row (i.e. each observation), set observation count to 1\n",
    "obs_temp_df['obs_count'] = 1\n",
    "obs_temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter observations data to only include observations from 1980-2020\n",
    "obs_temp_df = obs_temp_df.loc['1980-01-01':'2020-12-31']\n",
    "obs_temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate date list and dataframe for use later to pull segment-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create version of observations dataframe with date (as string) and segment id as indices\n",
    "obs_temp_daily_count = obs_temp_df.copy()\n",
    "obs_temp_daily_count.index = obs_temp_daily_count.index.astype(str)\n",
    "obs_temp_daily_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add segment id as second index level (for use later to pull observations for each segment)\n",
    "obs_temp_daily_count = obs_temp_daily_count.set_index('seg_id_nat', append=True)\n",
    "obs_temp_daily_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process spatial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_list = np.unique(segment_gdf['seg_id_nat'].tolist())\n",
    "segment_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get centroids for stream segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of dataframe\n",
    "centroid_gdf = segment_gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check crs of segment geodataframe\n",
    "centroid_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject before calculating segment centroids\n",
    "centroid_gdf = centroid_gdf.to_crs(epsg=6350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check crs of segment geodataframe\n",
    "centroid_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with centroid of each segment\n",
    "centroid_gdf['centroid'] = centroid_gdf['geometry'].interpolate(0.5, normalized=True)\n",
    "centroid_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign the geodataframe geometry to be the centroid column\n",
    "centroid_gdf = centroid_gdf.set_geometry('centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert to geographic crs (WGS 1984)\n",
    "centroid_gdf = centroid_gdf.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that centroid coordinates have been reprojected\n",
    "centroid_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column of only centroid latitude\n",
    "centroid_gdf['seg_centroid_lat'] = centroid_gdf['centroid'].apply(lambda p: p.y)\n",
    "centroid_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract latitudes of segment centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns except seg_id_nat and centroid\n",
    "segment_latitudes_df = centroid_gdf.drop(columns=['region','model_idx','InLine_FID','SmoLnFlag','geometry', 'centroid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set segment id as index\n",
    "segment_latitudes_df = segment_latitudes_df.set_index('seg_id_nat')\n",
    "segment_latitudes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataframe of unique sites in DRB with monitoring data to geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_drb_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframe to a geodataframe\n",
    "unique_drb_sites_gdf = gpd.GeoDataFrame(unique_drb_sites, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(unique_drb_sites.longitude, unique_drb_sites.latitude))\n",
    "unique_drb_sites_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_drb_sites_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = segment_gdf.plot(figsize=(30,15))\n",
    "unique_drb_sites_gdf.plot(ax = ax, markersize=2, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process temperature observations data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataframe of # of observations per year per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data w/ number of observations per year, by source\n",
    "n_obs_annual_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot into wide format\n",
    "source_annual_count = n_obs_annual_source.pivot(index='year', columns='source', values='n_obs')\n",
    "source_annual_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a continous date range from 1901-2020, at an annual interval\n",
    "year_index = pd.date_range('01-01-1960', '01-01-2021', freq='A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull only the year associated with each date\n",
    "year_index = year_index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the dataframe to fill in missing years\n",
    "# fill nas with 0s\n",
    "source_annual_count = source_annual_count.reindex(index=year_index, fill_value = 0)\n",
    "source_annual_count = source_annual_count.fillna(value = 0)\n",
    "source_annual_count = source_annual_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_annual_count.index.rename('year', inplace=True)\n",
    "source_annual_count = source_annual_count.rename(columns={'Other':'State or other agency'})\n",
    "source_annual_count = source_annual_count[['USGS', 'State or other agency']]\n",
    "source_annual_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get segment-specific counts of temperature observations for different time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count of all observations from 1980-2020 for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of all observations from 1980-2020 for each segment\n",
    "obs_temp_count = obs_temp_df.groupby(['seg_id_nat']).count()\n",
    "obs_temp_count = obs_temp_count.drop(columns=['subseg_id', 'mean_temp_c', 'min_temp_c', 'max_temp_c', 'site_id', 'year', 'year-month', 'month'])\n",
    "obs_temp_count = obs_temp_count.sort_index()\n",
    "obs_temp_count.index = obs_temp_count.index.astype(int)\n",
    "obs_temp_count = obs_temp_count.rename(columns={'obs_count':'total_count'})\n",
    "obs_temp_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count of observations in each year (from 1980-2020) for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of observations in each year, from 1980 - 2020\n",
    "obs_temp_year_count = obs_temp_df.groupby(['seg_id_nat', 'year']).count()\n",
    "obs_temp_year_count = obs_temp_year_count.drop(columns=['subseg_id', 'mean_temp_c', 'min_temp_c', 'max_temp_c', 'site_id', 'year-month', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count of all observations in 2019 for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset 2019 data from all temperature observations\n",
    "obs_temp_df_2019 = obs_temp_df.loc['2019-01-01':'2019-12-31']\n",
    "obs_temp_df_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of all observations in each month of 2019 for each segment\n",
    "obs_temp_df_2019_months = obs_temp_df_2019.copy()\n",
    "obs_temp_df_2019_months['month_name'] = obs_temp_df_2019_months.index.strftime('%B')\n",
    "obs_temp_count_month_2019 = obs_temp_df_2019_months.groupby(['seg_id_nat','month_name']).count()\n",
    "obs_temp_count_month_2019 = obs_temp_count_month_2019.drop(columns=['subseg_id','mean_temp_c','min_temp_c','max_temp_c','site_id','year','year-month','month'])\n",
    "obs_temp_mean_month_2019 = obs_temp_df_2019_months.groupby(['seg_id_nat','month_name']).mean()\n",
    "obs_temp_mean_month_2019 = obs_temp_mean_month_2019.drop(columns=['min_temp_c','max_temp_c','year','month','obs_count'])\n",
    "obs_temp_mean_month_2019 = obs_temp_mean_month_2019.rename(columns={'mean_temp_c':'mean_t_c'})\n",
    "obs_temp_month_2019 = obs_temp_count_month_2019.join(obs_temp_mean_month_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_month_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of all observations on each day in 2019 for each segment\n",
    "obs_temp_count_2019 = obs_temp_df_2019.groupby(['seg_id_nat']).count()\n",
    "obs_temp_count_2019 = obs_temp_count_2019.drop(columns=['month', 'year', 'subseg_id', 'mean_temp_c', 'min_temp_c', 'max_temp_c', 'site_id', 'year-month'])\n",
    "obs_temp_count_2019 = obs_temp_count_2019.sort_index()\n",
    "obs_temp_count_2019.index = obs_temp_count_2019.index.astype(int)\n",
    "obs_temp_count_2019 = obs_temp_count_2019.rename(columns={'obs_count':'total_count'})\n",
    "obs_temp_count_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get total annual and daily counts of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### annual counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group observations by year to get total count of observations in each year\n",
    "obs_annual_count = obs_temp_df.groupby(['year']).count()\n",
    "obs_annual_count = obs_annual_count.drop(columns=['subseg_id', 'seg_id_nat', 'mean_temp_c', 'min_temp_c', 'max_temp_c', 'site_id', 'year-month', 'month'])\n",
    "obs_annual_count = obs_annual_count.rename(columns={'obs_count': 'total_annual_count'})\n",
    "obs_annual_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### daily counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of observations on each day from 1980-2019\n",
    "obs_daily_count = obs_temp_df.groupby(['date']).count()\n",
    "obs_daily_count = obs_daily_count.drop(columns=['subseg_id', 'seg_id_nat', 'mean_temp_c', 'min_temp_c', 'max_temp_c', 'site_id', 'year', 'year-month', 'month'])\n",
    "obs_daily_count = obs_daily_count.rename(columns={'obs_count': 'total_daily_count'})\n",
    "obs_daily_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of observations on each day of 2019\n",
    "obs_daily_count_2019 = obs_daily_count.loc['2019-01-01':'2019-12-31']\n",
    "obs_daily_count_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data for matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of observations dataframe\n",
    "all_observations_df = obs_temp_df.copy()\n",
    "# set year as string type\n",
    "all_observations_df['year'] = all_observations_df['year'].astype(str)\n",
    "all_observations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### annual time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the segment ids as the columns\n",
    "matrix_annual_df = pd.DataFrame(columns=segment_list)\n",
    "# set a date range with annual timesteps from 1980-2020\n",
    "model_date_rng = pd.date_range('1980', periods=41, freq='A')\n",
    "# add a column to the dataframe with the set date range\n",
    "matrix_annual_df['Date'] = model_date_rng\n",
    "# convert dates to datetime format\n",
    "matrix_annual_df['Date'] = pd.to_datetime(matrix_annual_df['Date'])\n",
    "# set date as index\n",
    "matrix_annual_df = matrix_annual_df.set_index('Date')\n",
    "# create a column for year\n",
    "matrix_annual_df['year'] = matrix_annual_df.index.to_period('A')\n",
    "# set year as index\n",
    "matrix_annual_df = matrix_annual_df.set_index('year')\n",
    "# make index type string\n",
    "matrix_annual_df.index = matrix_annual_df.index.astype(str)\n",
    "matrix_annual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the dataframe columns to indices\n",
    "matrix_annual_series = matrix_annual_df.stack(dropna=False)\n",
    "matrix_annual_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the stacked series to a dataframe with two indices\n",
    "matrix_annual_stacked = matrix_annual_series.to_frame()\n",
    "# rename the second index to segment id\n",
    "matrix_annual_stacked.index = matrix_annual_stacked.index.rename('seg_id_nat', level=1)\n",
    "matrix_annual_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of observations for each segment in each year\n",
    "seg_obs_temp_year_count = all_observations_df.groupby(['year','seg_id_nat']).sum()\n",
    "seg_obs_temp_year_count = seg_obs_temp_year_count.drop(columns=['mean_temp_c','min_temp_c','max_temp_c','month'])\n",
    "seg_obs_temp_year_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the count for each segment to the matrix\n",
    "matrix_annual_obs = matrix_annual_stacked.join(seg_obs_temp_year_count, on=['year','seg_id_nat'])\n",
    "# drop empty column\n",
    "matrix_annual_obs = matrix_annual_obs.drop(columns=0)\n",
    "# replace na values with 0 (for 0 observations)\n",
    "matrix_annual_obs = matrix_annual_obs.fillna(0, axis=0)\n",
    "matrix_annual_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with total count for each segment (over whole period, from 1980-2020)\n",
    "matrix_annual_obs = matrix_annual_obs.join(obs_temp_count, on=['seg_id_nat'], how='left')\n",
    "matrix_annual_obs = matrix_annual_obs.fillna(0)\n",
    "matrix_annual_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of segment latitudes, ordered by latitude\n",
    "segment_latitudes_reindexed = segment_latitudes_df.sort_values(by='seg_centroid_lat')\n",
    "# reset the index\n",
    "segment_latitudes_reindexed = segment_latitudes_reindexed.reset_index()\n",
    "# set the segment id as the second index\n",
    "segment_latitudes_reindexed = segment_latitudes_reindexed.set_index('seg_id_nat', append=True)\n",
    "segment_latitudes_reindexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a column storing the first and second index levels as a tuple\n",
    "segment_latitudes_reindexed['index_tuple'] = segment_latitudes_reindexed.index\n",
    "# drop the first level of the index\n",
    "segment_latitudes_reindexed = segment_latitudes_reindexed.droplevel(level=0)\n",
    "segment_latitudes_reindexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with zero values named 'rank'\n",
    "segment_latitudes_reindexed['rank'] = 0\n",
    "segment_latitudes_reindexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the rank column with the first value of the index_tuple (to get rank of segment by latitude)\n",
    "for segment_id in segment_list:\n",
    "    segment_latitudes_reindexed['rank'][segment_id] = segment_latitudes_reindexed['index_tuple'][segment_id][0]\n",
    "segment_latitudes_reindexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe by the segment id\n",
    "segment_latitudes_reindexed = segment_latitudes_reindexed.sort_index()\n",
    "# drop the index tuple column\n",
    "segment_latitudes_reindexed = segment_latitudes_reindexed.drop(columns=['index_tuple'])\n",
    "segment_latitudes_reindexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the segment latitudes dataframe to the matrix of segment observations\n",
    "matrix_annual_obs = matrix_annual_obs.join(segment_latitudes_reindexed, on=['seg_id_nat'], how='left')\n",
    "matrix_annual_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the matrix by rank, so that the segment data is ordered by segment latitude\n",
    "matrix_annual_obs = matrix_annual_obs.sort_values(by='rank')\n",
    "# sort the matrix by year, so that data is ordered correctly temporally\n",
    "matrix_annual_obs = matrix_annual_obs.sort_index(level=0, sort_remaining=False)\n",
    "matrix_annual_obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### daily time interval - 2019 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe with the segment ids as the columns\n",
    "matrix_daily_2019_df = pd.DataFrame(columns=segment_list)\n",
    "# set up a date range with a daily timestep for the year 2019\n",
    "model_daily_2019_date_rng = pd.date_range('2019-01-01', periods=365, freq='D')\n",
    "# add a date column to the dataframe based on the date range\n",
    "matrix_daily_2019_df['date'] = model_daily_2019_date_rng\n",
    "# convert the date to datetime format\n",
    "matrix_daily_2019_df['date'] = pd.to_datetime(matrix_daily_2019_df['date'])\n",
    "# set the date as the index\n",
    "matrix_daily_2019_df = matrix_daily_2019_df.set_index('date')\n",
    "matrix_daily_2019_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the dataframe columns to indices\n",
    "matrix_daily_2019_series = matrix_daily_2019_df.stack(dropna=False)\n",
    "matrix_daily_2019_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with two index levels from the stacked series\n",
    "matrix_daily_2019_stacked = matrix_daily_2019_series.to_frame()\n",
    "# rename the second index level 'seg_id_nat'\n",
    "matrix_daily_2019_stacked.index = matrix_daily_2019_stacked.index.rename('seg_id_nat', level=1)\n",
    "matrix_daily_2019_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of observations for each segment on each day\n",
    "seg_obs_temp_daily_count = all_observations_df.groupby(['date','seg_id_nat']).sum()\n",
    "seg_obs_temp_daily_count = seg_obs_temp_daily_count.drop(columns=['mean_temp_c','min_temp_c','max_temp_c','month'])\n",
    "seg_obs_temp_daily_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the daily counts to the observation matrix\n",
    "matrix_daily_2019_obs = matrix_daily_2019_stacked.join(seg_obs_temp_daily_count, on=['date','seg_id_nat'])\n",
    "# drop empty column\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.drop(columns=0)\n",
    "# replace na values with 0s\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.fillna(0, axis=0)\n",
    "matrix_daily_2019_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with the total count for each segment (in 2019)\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.join(obs_temp_count_2019, on=['seg_id_nat'], how='left')\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.fillna(0)\n",
    "matrix_daily_2019_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull actual temperature observations\n",
    "obs_temp_daily_count_temps = all_observations_df.copy()\n",
    "obs_temp_daily_count_temps = obs_temp_daily_count_temps.set_index('seg_id_nat', append=True)\n",
    "obs_temp_daily_count_temps = obs_temp_daily_count_temps.drop(columns=['subseg_id','min_temp_c','max_temp_c','site_id','year','year-month','month','obs_count'])\n",
    "obs_temp_daily_count_temps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in temperature for each date for each segment\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.join(obs_temp_daily_count_temps, on=['date','seg_id_nat'])\n",
    "matrix_daily_2019_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latitude and latitude-based rank of each segment\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.join(segment_latitudes_reindexed, on=['seg_id_nat'], how='left')\n",
    "matrix_daily_2019_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe by rank\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.sort_values(by='rank')\n",
    "# sort dataframe by date\n",
    "matrix_daily_2019_obs = matrix_daily_2019_obs.sort_index(level=0, sort_remaining=False)\n",
    "matrix_daily_2019_obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed modeled segment outflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_outflow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime format\n",
    "seg_outflow['date'] = pd.to_datetime(seg_outflow['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date as index\n",
    "seg_outflow = seg_outflow.set_index(['date'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add year column\n",
    "seg_outflow['year'] = seg_outflow.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the data to a 30yr period\n",
    "seg_outflow_81_09 = seg_outflow.loc[(seg_outflow['year'] > 1980) & (seg_outflow['year'] < 2010)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average modeled outflow for each segment in each year\n",
    "seg_avg_outflow_81_09 = seg_outflow_81_09.groupby(['year']).mean()\n",
    "seg_avg_outflow_81_09.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataframe, so that the segment id is the index\n",
    "segindex_avg_outflow_81_09 = seg_avg_outflow_81_09.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with segment id\n",
    "segindex_avg_outflow_81_09['seg_id_nat'] = segindex_avg_outflow_81_09.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert segment id to integer type\n",
    "segindex_avg_outflow_81_09['seg_id_nat'] = segindex_avg_outflow_81_09['seg_id_nat'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with the overall average annual modeled outflow for each segment\n",
    "segindex_avg_outflow_81_09['avg_ann_flow'] = segindex_avg_outflow_81_09.mean(axis=1)\n",
    "segindex_avg_outflow_81_09.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to just segment id and average annual modeled segment outflow\n",
    "segment_maflow = segindex_avg_outflow_81_09[['seg_id_nat','avg_ann_flow']]\n",
    "segment_maflow = segment_maflow.set_index('seg_id_nat')\n",
    "segment_maflow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location for intermediate output data files\n",
    "intermediate_output_dir = 'intermediate_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intermediate output folder if it doesn't already exist\n",
    "if not os.path.exists(intermediate_output_dir):\n",
    "    os.mkdir(intermediate_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location for final output data files\n",
    "output_dir = '../public/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export spatial data that does not require processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate reservoir geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export geodataframe as a geojson\n",
    "reservoirs.to_file(os.path.join(intermediate_output_dir,'reservoirs.json'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate delaware bay geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delaware_bay_gdf.to_file(os.path.join(intermediate_output_dir,'NHDWaterbody_DelawareBay_pt6per_smooth.json'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export processed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### locations of unique monitoring sites with temperature observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert geodataframe to geojson\n",
    "unique_drb_sites_gdf.to_file(os.path.join(intermediate_output_dir,'unique_drb_sites.json'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count of observations at sites associated with each agency in each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_annual_count.to_csv(os.path.join(output_dir,'source_annual_count.csv'), index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### temporal counts of temperature observations (not segment-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export total counts for each year from 1980-2019\n",
    "obs_annual_count.to_csv(os.path.join(output_dir,'obs_annual_count.csv'), index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export total counts for each day of 2019\n",
    "obs_daily_count_2019.to_csv(os.path.join(output_dir,'obs_daily_count_2019.csv'), index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### matrix of segment temperature observations on annual timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_annual_obs.to_csv(os.path.join(output_dir,'matrix_annual_obs.csv'), index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### matrix of segment temperature observations on daily timestep (2019 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_daily_2019_obs.to_csv(os.path.join(output_dir,'matrix_daily_2019_obs.csv'), index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mean annual modeled segment outflow for each model segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_maflow.to_csv('TP_output_data/segment_maflow.csv', index_label='seg_id_nat')\n",
    "segment_maflow.to_csv(os.path.join(output_dir,'segment_maflow.csv'), index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct and export segment geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert segment geodataframe to dictionary format\n",
    "segment_polylines = segment_gdf.to_dict(orient='records')\n",
    "len(segment_polylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of years in record (1980-2014)\n",
    "year_list = np.unique(obs_temp_df['year'].tolist())\n",
    "year_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = ['January','February','March','April','May','June','July','August','September','October','November','December']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format designed to match desired structure of segmentDict\n",
    "# create empty array to store dictionaries\n",
    "segment_array = []\n",
    "# iterate through the list of segments to...\n",
    "i = 0\n",
    "while i < len(segment_polylines):\n",
    "    # create an empty segment dictionary\n",
    "    segment_dict = {}\n",
    "    # set type to Feature\n",
    "    segment_dict[\"type\"] = \"Feature\"\n",
    "    # set segment id field\n",
    "    segment_id = segment_polylines[i]['seg_id_nat']\n",
    "    segment_dict[\"seg_id_nat\"] = str(segment_id)\n",
    "    # add properties dictionary\n",
    "    segment_dict[\"properties\"] = {}\n",
    "    \n",
    "    # Add segment id as outer key\n",
    "    segment_dict[\"properties\"][\"seg_id_nat\"] = str(segment_id)\n",
    "    \n",
    "    # Add segment id as outer key\n",
    "    segment_dict[\"properties\"][str(segment_id)] = {}\n",
    "    \n",
    "    # add average annual flow\n",
    "    segment_dict[\"properties\"][str(segment_id)]['avg_ann_flow'] = str(segment_maflow['avg_ann_flow'][segment_id])\n",
    "      \n",
    "    # Add total count of observations in each segment\n",
    "    try:\n",
    "        segment_dict[\"properties\"][str(segment_id)][\"total_count\"] = str(obs_temp_count['total_count'][segment_id])\n",
    "    except:\n",
    "        segment_dict[\"properties\"][str(segment_id)][\"total_count\"] = '0'\n",
    "    \n",
    "    # create dictionary to store count for each year on record\n",
    "    segment_dict[\"properties\"][str(segment_id)][\"year_count\"] = {}\n",
    "    # iterate through years in list of years to...\n",
    "    for year in year_list:\n",
    "        # add the count of observations for each segment in that year\n",
    "        try:\n",
    "            segment_dict[\"properties\"][str(segment_id)][\"year_count\"][str(year)] = str(obs_temp_year_count['obs_count'][segment_id][year])\n",
    "        except:\n",
    "            segment_dict[\"properties\"][str(segment_id)][\"year_count\"][str(year)] = '0'\n",
    "            \n",
    "    # add dictionary to store monthly data from 2019 for each segment\n",
    "    segment_dict[\"properties\"][str(segment_id)][\"data_2019_monthly\"] = {}\n",
    "    for month in month_list:\n",
    "        try:\n",
    "            obs_temp_month_2019['obs_count'][segment_id][month]\n",
    "            segment_dict[\"properties\"][str(segment_id)][\"data_2019_monthly\"][month] = {}\n",
    "            segment_dict[\"properties\"][str(segment_id)][\"data_2019_monthly\"][month][\"month_count\"] = str(obs_temp_month_2019['obs_count'][segment_id][month])\n",
    "            segment_dict[\"properties\"][str(segment_id)][\"data_2019_monthly\"][month][\"month_avg_temp\"] = str(obs_temp_month_2019['mean_t_c'][segment_id][month])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # add geometry based on segment geometry\n",
    "    segment_dict[\"geometry\"] = segment_polylines[i][\"geometry\"]\n",
    "    \n",
    "    # append the segment dictionary to the empty segment array\n",
    "    segment_array.append(segment_dict)\n",
    "    \n",
    "    # print statement indicating progress\n",
    "    print(\"added segment\", str(i+1), \"of\", len(segment_polylines))\n",
    "    \n",
    "    # increase counter for loop\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty feature collection dictionary\n",
    "segment_feature_collection = {}\n",
    "segment_feature_collection[\"type\"] = \"FeatureCollection\"\n",
    "# set content of segment array as list of features within feature collection\n",
    "segment_feature_collection[\"features\"] = segment_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert segment feature collection to json format\n",
    "segment_geojson = geojson.dumps(segment_feature_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export formatted segment geojson\n",
    "with open(os.path.join(intermediate_output_dir,'segment_data.json'), 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(segment_geojson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert exported geojsons to topojsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requires installation of mapshaper: https://github.com/mbloch/mapshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get documentation\n",
    "! mapshaper -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reservoir json in intermediate_output folder to topojson w/ reduced precision and save in topojson subfolder of public data folder\n",
    "! mapshaper -i intermediate_output/reservoirs.json -o ../public/data/topojson/reservoirs.json format=topojson precision=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unique_drb_sites json in intermediate_output folder to topojson w/ reduced precision and save in topojson subfolder of public data folder\n",
    "! mapshaper -i intermediate_output/unique_drb_sites.json -o ../public/data/topojson/unique_drb_sites.json format=topojson precision=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert segment_geojson json in intermediate_output folder to topojson w/ reduced precision and save in topojson subfolder  of public data folder\n",
    "! mapshaper -i intermediate_output/segment_data.json -o ../public/data/topojson/segment_data.json format=topojson precision=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NHDWaterbody_DelawareBay_pt6per_smooth.json in intermediate_output folder to topojson w/ reduced precision and save in topojson subfolder of public data folder\n",
    "! mapshaper -i intermediate_output/NHDWaterbody_DelawareBay_pt6per_smooth.json -o ../public/data/topojson/DelawareBay.json format=topojson precision=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
